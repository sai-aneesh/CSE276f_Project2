{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opts: []\n",
      "env_kwargs: {}\n",
      "Observation space Dict()\n",
      "Action space Box(-1.0, 1.0, (8,), float32)\n",
      "Control mode pd_joint_delta_pos\n",
      "Reward mode normalized_dense\n",
      "reward tensor([0.0937])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([1], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0888])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([2], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0681])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([3], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0523])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([4], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0472])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([5], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0438])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([6], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0452])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([7], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0432])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([8], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0389])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([9], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0413])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([10], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0401])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([11], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0432])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([12], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0536])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([13], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0496])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([14], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0415])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([15], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0452])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([16], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0466])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([17], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0373])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([18], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0278])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([19], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0282])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([20], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0313])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([21], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0343])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([22], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0270])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([23], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0234])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([24], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0270])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([25], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0285])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([26], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0245])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([27], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0212])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([28], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0230])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([29], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0245])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([30], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0221])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([31], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0225])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([32], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0225])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([33], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0205])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([34], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0211])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([35], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0192])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([36], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0162])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([37], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0154])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([38], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0165])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([39], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0202])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([40], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0247])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([41], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0244])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([42], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0243])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([43], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0271])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([44], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0322])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([45], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0347])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([46], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0341])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([47], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0408])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([48], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0569])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([49], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.0741])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([50], dtype=torch.int32), 'success': tensor([False])}\n"
     ]
    }
   ],
   "source": [
    "!python -m mani_skill.examples.demo_random_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opts: []\n",
      "env_kwargs: {}\n",
      "Observation space Dict()\n",
      "Action space Box(-1.0, 1.0, (8,), float32)\n",
      "Control mode pd_joint_delta_pos\n",
      "Reward mode dense\n",
      "reward tensor([0.1492])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([1], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1495])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([2], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1502])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([3], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1526])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([4], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1517])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([5], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1490])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([6], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1472])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([7], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1479])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([8], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1462])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([9], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1466])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([10], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1450])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([11], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1450])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([12], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1434])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([13], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1420])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([14], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1398])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([15], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1400])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([16], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1406])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([17], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1423])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([18], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1420])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([19], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1429])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([20], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1423])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([21], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1419])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([22], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1436])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([23], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1448])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([24], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([25], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1436])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([26], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1434])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([27], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([28], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1431])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([29], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([30], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1425])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([31], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1412])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([32], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1404])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([33], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1402])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([34], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1411])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([35], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1407])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([36], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1415])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([37], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1418])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([38], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1415])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([39], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1421])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([40], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1435])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([41], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1464])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([42], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1454])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([43], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1449])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([44], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1445])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([45], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1450])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([46], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1434])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([47], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1423])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([48], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([49], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1432])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([50], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1460])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([51], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1488])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([52], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1480])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([53], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1476])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([54], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1477])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([55], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1459])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([56], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1473])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([57], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1490])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([58], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1508])\n",
      "terminated tensor([False])\n",
      "truncated tensor([False])\n",
      "info {'elapsed_steps': tensor([59], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1512])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([60], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1512])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([61], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1513])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([62], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1510])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([63], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1501])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([64], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1475])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([65], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1473])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([66], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1485])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([67], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1472])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([68], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1454])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([69], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1456])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([70], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1478])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([71], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1503])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([72], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1509])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([73], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1495])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([74], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1494])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([75], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1481])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([76], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1482])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([77], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1482])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([78], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1479])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([79], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1454])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([80], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([81], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1429])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([82], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1444])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([83], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1452])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([84], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1440])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([85], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1440])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([86], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1452])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([87], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1443])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([88], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1447])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([89], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([90], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1412])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([91], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1401])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([92], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1412])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([93], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1413])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([94], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1430])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([95], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1460])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([96], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1489])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([97], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1499])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([98], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1476])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([99], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1463])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([100], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1456])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([101], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1455])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([102], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1473])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([103], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1472])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([104], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1451])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([105], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1453])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([106], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1443])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([107], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1429])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([108], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1431])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([109], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1440])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([110], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([111], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1430])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([112], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([113], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1435])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([114], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1416])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([115], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1424])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([116], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1429])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([117], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1404])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([118], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1411])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([119], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1414])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([120], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1418])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([121], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1419])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([122], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1436])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([123], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1445])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([124], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1438])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([125], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([126], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([127], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([128], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1424])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([129], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1419])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([130], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1392])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([131], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1374])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([132], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1366])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([133], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1356])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([134], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1362])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([135], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1356])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([136], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1362])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([137], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1361])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([138], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1353])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([139], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1348])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([140], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1336])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([141], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1339])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([142], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1355])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([143], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1354])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([144], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1350])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([145], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1353])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([146], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1348])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([147], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1360])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([148], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1360])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([149], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1366])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([150], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1377])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([151], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1366])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([152], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1377])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([153], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1367])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([154], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1377])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([155], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1382])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([156], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1390])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([157], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1396])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([158], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1387])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([159], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1390])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([160], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1385])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([161], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1400])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([162], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1411])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([163], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1426])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([164], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1435])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([165], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1458])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([166], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1462])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([167], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1439])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([168], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1435])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([169], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1436])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([170], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1437])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([171], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1467])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([172], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1471])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([173], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1446])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([174], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1448])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([175], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1430])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([176], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([177], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1444])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([178], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1428])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([179], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1415])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([180], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1406])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([181], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1418])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([182], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1435])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([183], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1425])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([184], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1429])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([185], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1447])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([186], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1467])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([187], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1479])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([188], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1465])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([189], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1441])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([190], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1444])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([191], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1460])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([192], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1455])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([193], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1455])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([194], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1473])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([195], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1482])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([196], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1473])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([197], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1469])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([198], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1476])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([199], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1467])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([200], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1454])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([201], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1447])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([202], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1447])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([203], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1454])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([204], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1434])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([205], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1427])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([206], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1427])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([207], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1433])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([208], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1418])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([209], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1390])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([210], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1388])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([211], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1387])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([212], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1377])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([213], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1374])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([214], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1379])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([215], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1375])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([216], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1364])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([217], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1354])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([218], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1355])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([219], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1348])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([220], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1355])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([221], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1362])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([222], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1357])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([223], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1361])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([224], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1359])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([225], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1345])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([226], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1337])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([227], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1343])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([228], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1345])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([229], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1334])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([230], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1324])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([231], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([232], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1324])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([233], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1328])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([234], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1329])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([235], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1334])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([236], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1340])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([237], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1342])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([238], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1355])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([239], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1351])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([240], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1358])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([241], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1370])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([242], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1363])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([243], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1357])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([244], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1363])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([245], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1360])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([246], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1360])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([247], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1372])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([248], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1378])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([249], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1376])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([250], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1378])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([251], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1383])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([252], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1398])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([253], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1415])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([254], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1430])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([255], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1443])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([256], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1430])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([257], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1428])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([258], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1418])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([259], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1408])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([260], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1416])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([261], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1416])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([262], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1402])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([263], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1396])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([264], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1409])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([265], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1416])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([266], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1420])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([267], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1423])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([268], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1440])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([269], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1447])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([270], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1446])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([271], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1466])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([272], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1462])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([273], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1467])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([274], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1442])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([275], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1426])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([276], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1411])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([277], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1419])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([278], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1434])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([279], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1439])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([280], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1440])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([281], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1460])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([282], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1477])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([283], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1464])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([284], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1458])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([285], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1461])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([286], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1491])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([287], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1484])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([288], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1493])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([289], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1515])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([290], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1526])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([291], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1530])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([292], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1516])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([293], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1495])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([294], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1469])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([295], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1443])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([296], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1414])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([297], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1391])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([298], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1384])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([299], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1365])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([300], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1354])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([301], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1355])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([302], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1362])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([303], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1378])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([304], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1378])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([305], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1378])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([306], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1366])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([307], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1358])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([308], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1354])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([309], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1349])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([310], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1343])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([311], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([312], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1316])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([313], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([314], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([315], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1316])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([316], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1327])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([317], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1332])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([318], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1339])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([319], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1335])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([320], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1329])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([321], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1339])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([322], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1346])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([323], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1346])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([324], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1331])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([325], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([326], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1322])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([327], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1337])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([328], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1331])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([329], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1330])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([330], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1340])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([331], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1341])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([332], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1325])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([333], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1314])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([334], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([335], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([336], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([337], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([338], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([339], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([340], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([341], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([342], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([343], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([344], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([345], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([346], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([347], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([348], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([349], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1327])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([350], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([351], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([352], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([353], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([354], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([355], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([356], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([357], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([358], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([359], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([360], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([361], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([362], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([363], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([364], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([365], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([366], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([367], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([368], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([369], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([370], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1295])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([371], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([372], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1294])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([373], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([374], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([375], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([376], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1307])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([377], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([378], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([379], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([380], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([381], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([382], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([383], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([384], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1278])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([385], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([386], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([387], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([388], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1278])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([389], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([390], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1288])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([391], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1295])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([392], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1298])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([393], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([394], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1280])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([395], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1280])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([396], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([397], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1262])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([398], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([399], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([400], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1265])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([401], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1262])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([402], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([403], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1260])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([404], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([405], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1256])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([406], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1255])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([407], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([408], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1260])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([409], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1257])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([410], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([411], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([412], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([413], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([414], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1267])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([415], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1268])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([416], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1266])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([417], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([418], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([419], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1268])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([420], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([421], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([422], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([423], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([424], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([425], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([426], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([427], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([428], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([429], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([430], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1229])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([431], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([432], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([433], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([434], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([435], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([436], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([437], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([438], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([439], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1235])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([440], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1237])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([441], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([442], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([443], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([444], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([445], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([446], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([447], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([448], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([449], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1215])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([450], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([451], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([452], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([453], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([454], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([455], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([456], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([457], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1239])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([458], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1237])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([459], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([460], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1245])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([461], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([462], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([463], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([464], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([465], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1245])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([466], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1248])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([467], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([468], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([469], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1259])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([470], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([471], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1262])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([472], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([473], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1271])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([474], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([475], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1277])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([476], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([477], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([478], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([479], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([480], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([481], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([482], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([483], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([484], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([485], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([486], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([487], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([488], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([489], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([490], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([491], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([492], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([493], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([494], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1321])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([495], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1327])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([496], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([497], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([498], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([499], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([500], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([501], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([502], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([503], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([504], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([505], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([506], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1294])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([507], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([508], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([509], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([510], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1295])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([511], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([512], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([513], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([514], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([515], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([516], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1314])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([517], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([518], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([519], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1333])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([520], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1327])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([521], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([522], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1322])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([523], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1322])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([524], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1329])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([525], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1342])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([526], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1337])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([527], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1341])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([528], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1332])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([529], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1330])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([530], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1332])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([531], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1328])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([532], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1333])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([533], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1334])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([534], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([535], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([536], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([537], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([538], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([539], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([540], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1275])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([541], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1267])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([542], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([543], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([544], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1275])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([545], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([546], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([547], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1250])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([548], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1251])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([549], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1260])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([550], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1256])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([551], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([552], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1250])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([553], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1250])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([554], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([555], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1256])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([556], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([557], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1259])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([558], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1253])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([559], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([560], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1275])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([561], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([562], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([563], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([564], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([565], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([566], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([567], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([568], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([569], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1329])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([570], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([571], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([572], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([573], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([574], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([575], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([576], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([577], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([578], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([579], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1288])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([580], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1276])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([581], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([582], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1298])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([583], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1298])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([584], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([585], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([586], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([587], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([588], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([589], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([590], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([591], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([592], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([593], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([594], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([595], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([596], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([597], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([598], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([599], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([600], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([601], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([602], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1321])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([603], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1325])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([604], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1319])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([605], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([606], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([607], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([608], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([609], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1314])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([610], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1328])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([611], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1316])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([612], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([613], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([614], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([615], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1322])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([616], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1324])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([617], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1325])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([618], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1330])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([619], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([620], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1322])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([621], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([622], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([623], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1307])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([624], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([625], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([626], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([627], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([628], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([629], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1298])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([630], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([631], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([632], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([633], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([634], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([635], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([636], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([637], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([638], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([639], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([640], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1311])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([641], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1307])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([642], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([643], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([644], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([645], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1307])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([646], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1307])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([647], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([648], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([649], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([650], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([651], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([652], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([653], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([654], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([655], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([656], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([657], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([658], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([659], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([660], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1325])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([661], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1317])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([662], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1315])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([663], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([664], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([665], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1295])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([666], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([667], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([668], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1279])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([669], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1279])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([670], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1280])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([671], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([672], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([673], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([674], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([675], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([676], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([677], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([678], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1314])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([679], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1318])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([680], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1323])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([681], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([682], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1320])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([683], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1324])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([684], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1337])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([685], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1344])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([686], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1351])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([687], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1349])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([688], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1351])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([689], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1363])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([690], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1368])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([691], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1370])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([692], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1365])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([693], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1357])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([694], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1354])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([695], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1348])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([696], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1339])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([697], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1351])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([698], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1346])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([699], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1353])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([700], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1361])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([701], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1353])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([702], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1347])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([703], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1336])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([704], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1325])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([705], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1334])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([706], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1342])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([707], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1333])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([708], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1331])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([709], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1333])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([710], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1345])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([711], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1349])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([712], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1359])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([713], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1368])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([714], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1368])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([715], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1367])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([716], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1364])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([717], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1362])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([718], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1364])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([719], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1364])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([720], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1349])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([721], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1340])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([722], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1330])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([723], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1326])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([724], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1319])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([725], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([726], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([727], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1298])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([728], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([729], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1278])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([730], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([731], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([732], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([733], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1280])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([734], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([735], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1293])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([736], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([737], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([738], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([739], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([740], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1295])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([741], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([742], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1300])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([743], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([744], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1313])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([745], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1312])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([746], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1306])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([747], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([748], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1301])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([749], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([750], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1280])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([751], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1277])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([752], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1284])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([753], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([754], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([755], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([756], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1269])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([757], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([758], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1262])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([759], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1255])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([760], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([761], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1256])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([762], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([763], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1245])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([764], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1251])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([765], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1247])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([766], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1239])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([767], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1237])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([768], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1229])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([769], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1235])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([770], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([771], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([772], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([773], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([774], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([775], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([776], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([777], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1239])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([778], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([779], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([780], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1240])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([781], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([782], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([783], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([784], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([785], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([786], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([787], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1249])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([788], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1250])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([789], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([790], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([791], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([792], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([793], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([794], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1234])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([795], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1231])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([796], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([797], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([798], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([799], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([800], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1222])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([801], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([802], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1227])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([803], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([804], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1231])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([805], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1234])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([806], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1227])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([807], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([808], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([809], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([810], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([811], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([812], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1216])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([813], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([814], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1219])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([815], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([816], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([817], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([818], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1219])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([819], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([820], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([821], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1223])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([822], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1224])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([823], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([824], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1219])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([825], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1217])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([826], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1213])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([827], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1210])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([828], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1211])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([829], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1209])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([830], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1210])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([831], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1214])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([832], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1215])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([833], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1215])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([834], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1215])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([835], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1216])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([836], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([837], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1218])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([838], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1212])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([839], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1216])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([840], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1222])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([841], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([842], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([843], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1241])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([844], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1239])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([845], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1235])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([846], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1235])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([847], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([848], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1228])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([849], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([850], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([851], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1243])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([852], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([853], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([854], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([855], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([856], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1234])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([857], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([858], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1231])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([859], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([860], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([861], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1232])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([862], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([863], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([864], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1236])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([865], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([866], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1230])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([867], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([868], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1221])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([869], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1222])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([870], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1220])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([871], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1217])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([872], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1220])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([873], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([874], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1226])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([875], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1226])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([876], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1225])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([877], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1229])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([878], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1234])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([879], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([880], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1254])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([881], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1259])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([882], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1255])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([883], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1251])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([884], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1251])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([885], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1253])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([886], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([887], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1260])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([888], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([889], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1262])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([890], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([891], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1253])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([892], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([893], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1247])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([894], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1250])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([895], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1261])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([896], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1269])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([897], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([898], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([899], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([900], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1265])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([901], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1267])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([902], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([903], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([904], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1251])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([905], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([906], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([907], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1229])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([908], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1231])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([909], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1233])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([910], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1238])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([911], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1248])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([912], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1260])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([913], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([914], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1276])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([915], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([916], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([917], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([918], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([919], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1266])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([920], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([921], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1274])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([922], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([923], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1288])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([924], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1291])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([925], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([926], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([927], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([928], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([929], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1294])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([930], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([931], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([932], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([933], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([934], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1288])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([935], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1285])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([936], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([937], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1288])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([938], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1282])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([939], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1272])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([940], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([941], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1283])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([942], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1275])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([943], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1276])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([944], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1284])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([945], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([946], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([947], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([948], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([949], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1304])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([950], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([951], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1299])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([952], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([953], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1292])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([954], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([955], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1290])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([956], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1289])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([957], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([958], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([959], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([960], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1286])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([961], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1294])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([962], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([963], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1314])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([964], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([965], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([966], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([967], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1309])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([968], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1303])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([969], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1302])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([970], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1305])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([971], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1308])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([972], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1310])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([973], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1297])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([974], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1296])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([975], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1287])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([976], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1284])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([977], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1281])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([978], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1273])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([979], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([980], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1259])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([981], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([982], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1253])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([983], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([984], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1265])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([985], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1270])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([986], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([987], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1263])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([988], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1264])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([989], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1258])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([990], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1249])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([991], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([992], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1242])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([993], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1244])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([994], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1246])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([995], dtype=torch.int32), 'success': tensor([False])}\n",
      "reward tensor([0.1252])\n",
      "terminated tensor([False])\n",
      "truncated True\n",
      "info {'elapsed_steps': tensor([996], dtype=torch.int32), 'success': tensor([False])}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/mani_skill/examples/demo_random_action.py\", line 98, in <module>\n",
      "    main(parse_args())\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/mani_skill/examples/demo_random_action.py\", line 86, in main\n",
      "    env.render()\n",
      "  File \"/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py\", line 471, in render\n",
      "    return self.env.render()\n",
      "  File \"/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py\", line 70, in render\n",
      "    return self.env.render(*args, **kwargs)\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/mani_skill/envs/sapien_env.py\", line 1111, in render\n",
      "    return self.render_human()\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/mani_skill/envs/sapien_env.py\", line 1047, in render_human\n",
      "    self._viewer.render()\n",
      "  File \"/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/sapien/utils/viewer/viewer.py\", line 203, in render\n",
      "    if self.window.should_close:\n",
      "AttributeError: 'NoneType' object has no attribute 'should_close'\n"
     ]
    }
   ],
   "source": [
    "!python -m mani_skill.examples.demo_random_action -e RollBall-v1 --render-mode=\"human\" # run with A GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------------------------------------------- #\n",
      "Task ID: PickCube-v1, 1024 parallel environments, sim_backend=gpu\n",
      "obs_mode=state, control_mode=pd_joint_delta_pos\n",
      "render_mode=sensors, sensor_details=RGBD(128x128)\n",
      "sim_freq=100, control_freq=20\n",
      "observation space: Box(-inf, inf, (1024, 42), float32)\n",
      "(single) action space: Box(-1.0, 1.0, (8,), float32)\n",
      "# -------------------------------------------------------------------------- #\n",
      "start recording env.step metrics\n",
      "env.step: 29645.479 steps/s, 28.951 parallel steps/s, 100 steps in 3.454s\n",
      "start recording env.step+env.reset metrics\n",
      "env.step+env.reset: 28819.340 steps/s, 28.144 parallel steps/s, 1000 steps in 35.532s\n"
     ]
    }
   ],
   "source": [
    "!python -m mani_skill.examples.benchmarking.gpu_sim --num-envs=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field wandb_entity is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field checkpoint is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "Running training\n",
      "Saving eval videos to runs/RollBall-v1__ppo__1__1717888144/videos\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.max_episode_steps to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.max_episode_steps` for environment variables or `env.get_wrapper_attr('max_episode_steps')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "####\n",
      "args.num_iterations=39 args.num_envs=1024 args.num_eval_envs=8\n",
      "args.minibatch_size=1600 args.batch_size=51200 args.update_epochs=8\n",
      "####\n",
      "Epoch: 1, global_step=0\n",
      "Evaluating\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/examples/baselines/ppo/ppo.py\", line 270, in <module>\n",
      "    returns = np.concatenate(returns)\n",
      "ValueError: need at least one array to concatenate\n"
     ]
    }
   ],
   "source": [
    "!python ppo.py --env_id=\"RollBall-v1\" \\\n",
    "  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \\\n",
    "  --total_timesteps=2_000_000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field wandb_entity is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field checkpoint is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "Running training\n",
      "Saving eval videos to runs/RollBall-v1__ppo__100__1717888477/videos\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.max_episode_steps to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.max_episode_steps` for environment variables or `env.get_wrapper_attr('max_episode_steps')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "####\n",
      "args.num_iterations=162 args.num_envs=1024 args.num_eval_envs=8\n",
      "args.minibatch_size=1920 args.batch_size=61440 args.update_epochs=8\n",
      "####\n",
      "Epoch: 1, global_step=0\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=7.906454086303711\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_1.pt\n",
      "SPS: 11773\n",
      "Epoch: 2, global_step=61440\n",
      "SPS: 15754\n",
      "Epoch: 3, global_step=122880\n",
      "SPS: 17761\n",
      "Epoch: 4, global_step=184320\n",
      "SPS: 18782\n",
      "Epoch: 5, global_step=245760\n",
      "SPS: 19441\n",
      "Epoch: 6, global_step=307200\n",
      "SPS: 19888\n",
      "Epoch: 7, global_step=368640\n",
      "SPS: 20169\n",
      "Epoch: 8, global_step=430080\n",
      "SPS: 20477\n",
      "Epoch: 9, global_step=491520\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=9.776786804199219\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_9.pt\n",
      "SPS: 19200\n",
      "Epoch: 10, global_step=552960\n",
      "SPS: 19508\n",
      "Epoch: 11, global_step=614400\n",
      "SPS: 19807\n",
      "Epoch: 12, global_step=675840\n",
      "SPS: 20037\n",
      "Epoch: 13, global_step=737280\n",
      "SPS: 20246\n",
      "Epoch: 14, global_step=798720\n",
      "SPS: 20424\n",
      "Epoch: 15, global_step=860160\n",
      "SPS: 20594\n",
      "Epoch: 16, global_step=921600\n",
      "SPS: 20735\n",
      "Epoch: 17, global_step=983040\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=9.659099578857422\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_17.pt\n",
      "SPS: 19957\n",
      "Epoch: 18, global_step=1044480\n",
      "SPS: 20122\n",
      "Epoch: 19, global_step=1105920\n",
      "SPS: 20248\n",
      "Epoch: 20, global_step=1167360\n",
      "SPS: 20350\n",
      "Epoch: 21, global_step=1228800\n",
      "SPS: 20453\n",
      "Epoch: 22, global_step=1290240\n",
      "SPS: 20326\n",
      "Epoch: 23, global_step=1351680\n",
      "SPS: 20192\n",
      "Epoch: 24, global_step=1413120\n",
      "SPS: 20133\n",
      "Epoch: 25, global_step=1474560\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=10.033769607543945\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_25.pt\n",
      "SPS: 19440\n",
      "Epoch: 26, global_step=1536000\n",
      "SPS: 19447\n",
      "Epoch: 27, global_step=1597440\n",
      "SPS: 19489\n",
      "Epoch: 28, global_step=1658880\n",
      "SPS: 19570\n",
      "Epoch: 29, global_step=1720320\n",
      "SPS: 19620\n",
      "Epoch: 30, global_step=1781760\n",
      "SPS: 19649\n",
      "Epoch: 31, global_step=1843200\n",
      "SPS: 19572\n",
      "Epoch: 32, global_step=1904640\n",
      "SPS: 19511\n",
      "Epoch: 33, global_step=1966080\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=10.091805458068848\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_33.pt\n",
      "SPS: 19038\n",
      "Epoch: 34, global_step=2027520\n",
      "SPS: 19030\n",
      "Epoch: 35, global_step=2088960\n",
      "SPS: 19040\n",
      "Epoch: 36, global_step=2150400\n",
      "SPS: 19118\n",
      "Epoch: 37, global_step=2211840\n",
      "SPS: 19186\n",
      "Epoch: 38, global_step=2273280\n",
      "SPS: 19230\n",
      "Epoch: 39, global_step=2334720\n",
      "SPS: 19170\n",
      "Epoch: 40, global_step=2396160\n",
      "SPS: 19114\n",
      "Epoch: 41, global_step=2457600\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=10.54245662689209\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_41.pt\n",
      "SPS: 18759\n",
      "Epoch: 42, global_step=2519040\n",
      "SPS: 18723\n",
      "Epoch: 43, global_step=2580480\n",
      "SPS: 18692\n",
      "Epoch: 44, global_step=2641920\n",
      "SPS: 18727\n",
      "Epoch: 45, global_step=2703360\n",
      "SPS: 18798\n",
      "Epoch: 46, global_step=2764800\n",
      "SPS: 18872\n",
      "Epoch: 47, global_step=2826240\n",
      "SPS: 18879\n",
      "Epoch: 48, global_step=2887680\n",
      "SPS: 18917\n",
      "Epoch: 49, global_step=2949120\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=12.29390811920166\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_49.pt\n",
      "SPS: 18719\n",
      "Epoch: 50, global_step=3010560\n",
      "SPS: 18773\n",
      "Epoch: 51, global_step=3072000\n",
      "SPS: 18871\n",
      "Epoch: 52, global_step=3133440\n",
      "SPS: 18897\n",
      "Epoch: 53, global_step=3194880\n",
      "SPS: 18917\n",
      "Epoch: 54, global_step=3256320\n",
      "SPS: 18951\n",
      "Epoch: 55, global_step=3317760\n",
      "SPS: 19000\n",
      "Epoch: 56, global_step=3379200\n",
      "SPS: 19039\n",
      "Epoch: 57, global_step=3440640\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=16.166433334350586\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_57.pt\n",
      "SPS: 18866\n",
      "Epoch: 58, global_step=3502080\n",
      "SPS: 18870\n",
      "Epoch: 59, global_step=3563520\n",
      "SPS: 18896\n",
      "Epoch: 60, global_step=3624960\n",
      "SPS: 18923\n",
      "Epoch: 61, global_step=3686400\n",
      "SPS: 18946\n",
      "Epoch: 62, global_step=3747840\n",
      "SPS: 19013\n",
      "Epoch: 63, global_step=3809280\n",
      "SPS: 19032\n",
      "Epoch: 64, global_step=3870720\n",
      "SPS: 19038\n",
      "Epoch: 65, global_step=3932160\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=15.676131248474121\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_65.pt\n",
      "SPS: 18854\n",
      "Epoch: 66, global_step=3993600\n",
      "SPS: 18872\n",
      "Epoch: 67, global_step=4055040\n",
      "SPS: 18920\n",
      "Epoch: 68, global_step=4116480\n",
      "SPS: 18969\n",
      "Epoch: 69, global_step=4177920\n",
      "SPS: 19007\n",
      "Epoch: 70, global_step=4239360\n",
      "SPS: 19002\n",
      "Epoch: 71, global_step=4300800\n",
      "SPS: 19012\n",
      "Epoch: 72, global_step=4362240\n",
      "SPS: 19041\n",
      "Epoch: 73, global_step=4423680\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=15.922426223754883\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_73.pt\n",
      "SPS: 18906\n",
      "Epoch: 74, global_step=4485120\n",
      "SPS: 18958\n",
      "Epoch: 75, global_step=4546560\n",
      "SPS: 18958\n",
      "Epoch: 76, global_step=4608000\n",
      "SPS: 18961\n",
      "Epoch: 77, global_step=4669440\n",
      "SPS: 18979\n",
      "Epoch: 78, global_step=4730880\n",
      "SPS: 18992\n",
      "Epoch: 79, global_step=4792320\n",
      "SPS: 19016\n",
      "Epoch: 80, global_step=4853760\n",
      "SPS: 19011\n",
      "Epoch: 81, global_step=4915200\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.0\n",
      "eval_episodic_return=15.284778594970703\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_81.pt\n",
      "SPS: 18867\n",
      "Epoch: 82, global_step=4976640\n",
      "SPS: 18881\n",
      "Epoch: 83, global_step=5038080\n",
      "SPS: 18907\n",
      "Epoch: 84, global_step=5099520\n",
      "SPS: 18939\n",
      "Epoch: 85, global_step=5160960\n",
      "SPS: 18917\n",
      "Epoch: 86, global_step=5222400\n",
      "SPS: 18920\n",
      "Epoch: 87, global_step=5283840\n",
      "SPS: 18923\n",
      "Epoch: 88, global_step=5345280\n",
      "SPS: 18949\n",
      "Epoch: 89, global_step=5406720\n",
      "Evaluating\n",
      "Evaluated 480 steps resulting in 8 episodes\n",
      "eval_success_rate=0.125\n",
      "eval_episodic_return=19.57698631286621\n",
      "model saved to runs/RollBall-v1__ppo__100__1717888477/ckpt_89.pt\n",
      "SPS: 18836\n",
      "Epoch: 90, global_step=5468160\n",
      "SPS: 18834\n",
      "Epoch: 91, global_step=5529600\n",
      "SPS: 18842\n",
      "Epoch: 92, global_step=5591040\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aneesh/aneesh/ucsd_q3/cse276f/project2/CSE276f_Project2/examples/baselines/ppo/ppo.py\", line 391, in <module>\n",
      "    _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
      "RuntimeError: CUDA error: unspecified launch failure\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[2024-06-08 16:21:00.913] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] Mem free failed with error code 719!\n",
      "\n",
      "[2024-06-08 16:21:00.913] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.913] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.913] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.913] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.919] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] Mem free failed with error code 719!\n",
      "\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n",
      "[2024-06-08 16:21:00.964] [SAPIEN] [\u001b[1m\u001b[41mcritical\u001b[m] /buildAgent/work/eb2f45c4acc808a0/physx/source/gpucommon/src/PxgCudaMemoryAllocator.cpp\n"
     ]
    }
   ],
   "source": [
    "!python ppo.py --env_id=\"RollBall-v1\"  --num_envs=1024 --update_epochs=8 --num_minibatches=32 --seed=100 --total_timesteps=100_00_000 --eval_freq=8 --num-steps=60 --num_eval_steps=60 --gamma 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field wandb_entity is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/tyro/_fields.py:343: UserWarning: The field checkpoint is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
      "  warnings.warn(\n",
      "Running evaluation\n",
      "Saving eval videos to runs/PushCube-v1__ppo__1__1717887181/test_videos\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.max_episode_steps to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.max_episode_steps` for environment variables or `env.get_wrapper_attr('max_episode_steps')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aneesh/anaconda3/envs/cs276f/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.max_episode_steps to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.max_episode_steps` for environment variables or `env.get_wrapper_attr('max_episode_steps')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "####\n",
      "args.num_iterations=390 args.num_envs=512 args.num_eval_envs=1\n",
      "args.minibatch_size=800 args.batch_size=25600 args.update_epochs=4\n",
      "####\n",
      "Epoch: 1, global_step=0\n",
      "Evaluating\n",
      "Evaluated 1000 steps resulting in 39 episodes\n",
      "eval_success_rate=0.8717948717948718\n",
      "eval_episodic_return=8.686422348022461\n"
     ]
    }
   ],
   "source": [
    "!python ppo.py --env_id=\"PushCube-v1\" \\\n",
    "   --evaluate --checkpoint=runs/PushCube-v1__ppo__1__1717887181/final_ckpt.pt \\\n",
    "   --num_eval_envs=1 --num-eval-steps=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
